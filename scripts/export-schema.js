#!/usr/bin/env node/** * This script exports the database schema and data from your local MySQL * to prepare for migration to PlanetScale. *  * Usage: * node export-schema.js */const fs = require('fs');const path = require('path');const mysql = require('mysql2/promise');const dotenv = require('dotenv');const { promisify } = require('util');const exec = promisify(require('child_process').exec);// Load environment variablesdotenv.config();// MySQL connection configconst dbConfig = {  host: process.env.MYSQL_HOST || 'localhost',  user: process.env.MYSQL_USER || 'root',  password: process.env.MYSQL_PASS || '',  database: process.env.MYSQL_DATABASE || 'bashvote',  port: process.env.MYSQL_PORT || 3306};// Output directoryconst exportDir = path.join(__dirname, '..', 'migrations');const dataDir = path.join(exportDir, 'data');// Ensure export directories existif (!fs.existsSync(exportDir)) {  fs.mkdirSync(exportDir, { recursive: true });}if (!fs.existsSync(dataDir)) {  fs.mkdirSync(dataDir, { recursive: true });}async function exportSchema() {  console.log('Exporting database schema...');    try {    const schemaFile = path.join(exportDir, 'schema.sql');        // Using mysqldump to export schema only    const { stdout, stderr } = await exec(      `mysqldump -h ${dbConfig.host} -P ${dbConfig.port} -u ${dbConfig.user}${dbConfig.password ? ` -p${dbConfig.password}` : ''} --no-data ${dbConfig.database} > ${schemaFile}`    );        if (stderr && !stderr.includes('Warning')) {      console.error('Error exporting schema:', stderr);      return false;    }        console.log(`Schema exported to ${schemaFile}`);    return true;  } catch (error) {    console.error('Failed to export schema:', error);    return false;  }}async function exportData() {  console.log('Exporting table data...');    try {    const connection = await mysql.createConnection(dbConfig);        // Get all tables    const [tables] = await connection.execute(      `SELECT TABLE_NAME FROM information_schema.TABLES        WHERE TABLE_SCHEMA = ?       AND TABLE_TYPE = 'BASE TABLE'       AND TABLE_NAME != 'sessions'`, // Skip sessions table      [dbConfig.database]    );        for (const tableRow of tables) {      const tableName = tableRow.TABLE_NAME;      console.log(`Exporting data from table ${tableName}...`);            // Get data from table      const [rows] = await connection.execute(`SELECT * FROM ${tableName}`);            if (rows.length === 0) {        console.log(`- No data in table ${tableName}. Skipping.`);        continue;      }            // Save to JSON file      const outputFile = path.join(dataDir, `${tableName}.json`);      fs.writeFileSync(outputFile, JSON.stringify(rows, null, 2));      console.log(`- Exported ${rows.length} rows to ${outputFile}`);    }        await connection.end();    console.log('Data export complete');    return true;  } catch (error) {    console.error('Failed to export data:', error);    return false;  }}async function createImportScript() {  console.log('Creating import script...');    const scriptContent = `#!/usr/bin/env node/** * This script imports data from JSON files into your PlanetScale database. * Make sure your .env file is configured with PlanetScale credentials. *  * Usage: * node import-data.js */const fs = require('fs');const path = require('path');const mysql = require('mysql2/promise');const dotenv = require('dotenv');// Load environment variablesdotenv.config();// MySQL connection config (PlanetScale)const dbConfig = {  host: process.env.MYSQL_HOST || 'localhost',  user: process.env.MYSQL_USER || 'root',  password: process.env.MYSQL_PASS || '',  database: process.env.MYSQL_DATABASE || 'bashvote',  port: process.env.MYSQL_PORT || 3306,  ssl: process.env.MYSQL_SSL ? { rejectUnauthorized: true } : undefined};// Data directoryconst dataDir = path.join(__dirname, 'data');async function importData() {  console.log('Starting data import to PlanetScale...');    try {    // Connect to PlanetScale    console.log('Connecting to PlanetScale...');    const connection = await mysql.createConnection(dbConfig);    console.log('Connected to PlanetScale database');        // Get all JSON files in migrations directory (except schema.sql)    const files = fs.readdirSync(dataDir)      .filter(file => file.endsWith('.json'));        // Process each table    for (const file of files) {      const tableName = path.basename(file, '.json');      const filePath = path.join(dataDir, file);            console.log(\`Importing data for table \${tableName}...\`);            try {        // Read the data from the JSON file        const data = JSON.parse(fs.readFileSync(filePath, 'utf8'));                if (data.length === 0) {          console.log(\`- No data to import for \${tableName}. Skipping.\`);          continue;        }                // Get column names from the first object        const columns = Object.keys(data[0]);                // Import data in batches to avoid query size limits        const batchSize = 100;        let processed = 0;                for (let i = 0; i < data.length; i += batchSize) {          const batch = data.slice(i, i + batchSize);                    // Prepare the placeholders for each row          const placeholders = batch.map(() =>             '(' + columns.map(() => '?').join(',') + ')'          ).join(',');                    // Flatten the values for the query          const values = batch.flatMap(row =>             columns.map(col => row[col])          );                    // Build the query          const query = \`INSERT INTO \${tableName} (\${columns.join(',')}) VALUES \${placeholders}\`;                    // Execute the query          const [result] = await connection.execute(query, values);          processed += batch.length;          console.log(\`- Imported \${processed}/\${data.length} rows into \${tableName}\`);        }      } catch (error) {        console.error(\`Error importing data for \${tableName}:\`, error);      }    }        await connection.end();    console.log('Data import complete');  } catch (error) {    console.error('Failed to import data:', error);  }}importData();`;  const importScriptPath = path.join(exportDir, 'import-data.js');  fs.writeFileSync(importScriptPath, scriptContent);  fs.chmodSync(importScriptPath, '755');  console.log(`Import script created at ${importScriptPath}`);  return true;}async function run() {  const schemaExported = await exportSchema();  const dataExported = await exportData();    if (schemaExported && dataExported) {    await createImportScript();        console.log('\n===== MIGRATION PREPARATION COMPLETE =====');    console.log('\nNext steps:');    console.log('1. Create a PlanetScale database at https://planetscale.com');    console.log('2. Connect to your PlanetScale database and run the schema.sql');    console.log('   (You can use the PlanetScale CLI or web console for this)');    console.log('3. Update your .env file with PlanetScale credentials');    console.log('4. Run the import script:');    console.log('   node migrations/import-data.js');    console.log('\nHappy migrating!\n');  } else {    console.error('Migration preparation failed. Please fix the errors and try again.');  }}run();
